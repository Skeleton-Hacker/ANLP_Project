==========================================
SLURM_JOB_ID = 2552996
SLURM_NODELIST = gnode006
SLURM_JOB_GPUS = 0,1,2,3
==========================================
==========================================
SLURM_JOB_ID = 2552996
SLURM_NODELIST = gnode006
SLURM_JOB_GPUS = 0,1,2,3
==========================================
Micromamba environment activated!
Python 3.10.19
/home2/gr/mamba/envs/next-level-lm/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
2025-12-11 22:09:00,646 - INFO - Loading test dataset...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
2025-12-11 22:09:05,780 - INFO - Loaded 355 samples from chunked_data/test_chunks_encoded.pkl
2025-12-11 22:09:05,780 - INFO - Using device: cuda
2025-12-11 22:09:08,401 - INFO - Configured Gemini model: gemini-2.5-flash
2025-12-11 22:09:08,401 - INFO - Loading finetuned T5 model...
2025-12-11 22:09:10,038 - INFO - Total params: 224,306,688
2025-12-11 22:09:10,038 - INFO - Trainable params: 224,306,688
2025-12-11 22:09:11,270 - INFO - Loading base T5 model...
2025-12-11 22:09:13,164 - INFO - Evaluating 25 samples serially...
Processing samples:   0%|          | 0/25 [00:00<?, ?it/s]2025-12-11 22:09:13,169 - INFO - Processing sample 327 (story_id: ef92e6a5b6fe08813c84e8349ddd2cb2dc842bc7)
2025-12-11 22:09:16,632 - INFO - Finetuned T5 generated (length=594)
2025-12-11 22:09:19,147 - INFO - Base T5 generated (length=267)
2025-12-11 22:09:19,147 - INFO - Waiting 10s before Gemini call...
2025-12-11 22:09:29,157 - INFO - Calling Gemini API...
2025-12-11 22:09:38,479 - INFO - Gemini generated summary (length=224)
2025-12-11 22:09:38,479 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:09:41,447 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:09:44,318 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing samples:   4%|▍         | 1/25 [00:33<13:35, 33.98s/it]2025-12-11 22:09:47,155 - INFO - Processing sample 57 (story_id: 22d1af3e29dc599ec0634fe860359027adea6944)
2025-12-11 22:09:50,397 - INFO - Finetuned T5 generated (length=575)
2025-12-11 22:09:52,150 - INFO - Base T5 generated (length=224)
2025-12-11 22:09:52,150 - INFO - Waiting 10s before Gemini call...
2025-12-11 22:10:02,160 - INFO - Calling Gemini API...
2025-12-11 22:10:21,625 - INFO - Gemini generated summary (length=282)
2025-12-11 22:10:21,626 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:10:24,550 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:10:27,420 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing samples:   8%|▊         | 2/25 [01:17<15:05, 39.38s/it]2025-12-11 22:10:30,314 - INFO - Processing sample 12 (story_id: 069ba0ceb78ef8fe8a193b7b4f225938f2044f0c)
2025-12-11 22:10:33,595 - INFO - Finetuned T5 generated (length=483)
2025-12-11 22:10:36,118 - INFO - Base T5 generated (length=394)
2025-12-11 22:10:36,119 - INFO - Waiting 10s before Gemini call...
2025-12-11 22:10:46,129 - INFO - Calling Gemini API...
2025-12-11 22:10:59,815 - INFO - Gemini generated summary (length=326)
2025-12-11 22:10:59,816 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:11:02,792 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:11:05,651 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing samples:  12%|█▏        | 3/25 [01:55<14:18, 39.02s/it]2025-12-11 22:11:08,923 - INFO - Processing sample 140 (story_id: 5a3c3fa94456d41df5d8711b33687c39f2aa1ca8)
2025-12-11 22:11:12,163 - INFO - Finetuned T5 generated (length=548)
2025-12-11 22:11:13,465 - INFO - Base T5 generated (length=162)
2025-12-11 22:11:13,465 - INFO - Waiting 10s before Gemini call...
2025-12-11 22:11:23,475 - INFO - Calling Gemini API...
2025-12-11 22:11:39,515 - INFO - Gemini generated summary (length=234)
2025-12-11 22:11:39,515 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:11:42,450 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:11:45,298 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing samples:  16%|█▌        | 4/25 [02:34<13:41, 39.11s/it]2025-12-11 22:11:48,148 - INFO - Processing sample 125 (story_id: 4ff62e050b4eba11e273a060f0e5b0080746ea79)
2025-12-11 22:11:51,419 - INFO - Finetuned T5 generated (length=578)
2025-12-11 22:11:53,042 - INFO - Base T5 generated (length=161)
2025-12-11 22:11:53,043 - INFO - Waiting 10s before Gemini call...
2025-12-11 22:12:03,053 - INFO - Calling Gemini API...
2025-12-11 22:12:29,906 - INFO - Gemini generated summary (length=224)
2025-12-11 22:12:29,907 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:12:32,798 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:12:35,914 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing samples:  20%|██        | 5/25 [03:25<14:25, 43.26s/it]2025-12-11 22:12:38,766 - INFO - Processing sample 114 (story_id: 46e4a4aa0f4ba89cf4d4972f1b9f6657f56e8f63)
2025-12-11 22:12:41,947 - INFO - Finetuned T5 generated (length=576)
2025-12-11 22:12:43,527 - INFO - Base T5 generated (length=172)
2025-12-11 22:12:43,527 - INFO - Waiting 10s before Gemini call...
2025-12-11 22:12:53,537 - INFO - Calling Gemini API...
2025-12-11 22:13:02,088 - INFO - Gemini generated summary (length=231)
2025-12-11 22:13:02,089 - INFO - Using default tokenizer.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-11 22:13:04,982 - INFO - Using default tokenizer.
